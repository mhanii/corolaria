# Coloraria Configuration
# All parameters externalized for easy tuning

# Database Configuration
database:
  type: "mariadb"  # "sqlite" | "mariadb" (override with DATABASE_TYPE env var)
  
  # SQLite (for local dev/fallback)
  sqlite:
    path: "data/coloraria.db"
  
  # MariaDB (for production & concurrent access)
  mariadb:
    host: "mariadb"  # Docker service name
    port: 3306
    database: "coloraria"
    user: "coloraria_user"
    password: ""  # Set via MARIADB_PASSWORD env var
    pool_size: 10
    pool_recycle: 3600  # Recycle connections after 1 hour
    # Connection string can also be set via MARIADB_URI env var

llm:
  # Legacy single-provider config (used if resilient mode disabled)
  provider: "gemini"
  model: "gemini-2.0-flash"
  temperature: 1
  max_tokens: 10240000
  
  # Resilient multi-provider mode
  resilient:
    enabled: true  # Set to false to use single provider above
    
    # Main provider (most used, reliable)
    main:
      provider: "gemini"
      model: "gemini-2.5-flash"
      temperature: 1
      retries: 3
      delays: [2, 4, 8]  # seconds between retries
    
    # Backup provider (used if main fails)
    backup:
      provider: "gemini"
      model: "gemini-2.0-flash"
      temperature: 1
      retries: 2
      delays: [2, 4]
    
    # Fallback provider (last resort, most reliable)
    fallback:
      provider: "azure_openai"
      model: "gpt-5-mini"
      temperature: 1

retrieval:
  top_k: 10
  strategy: "vector"
  index_name: "article_embeddings"
  min_score: 0.0  # No minimum score filter by default
  max_refs: 0     # Maximum REFERS_TO articles to include per chunk

citations:
  format: "[{index}]"
  include_in_response: true
  max_citations: 20

conversation:
  max_history_messages: 10  # Keep last N messages for context
  max_context_tokens: 40000  # Approximate token limit for context

version_context:
  # How many next versions to include recursively (0 = none, -1 = all)
  next_version_depth: 0  # Get all subsequent versions
  # How many previous versions to include (0 = none)
  previous_version_depth: 0  # Get only immediate previous version

# Observability Configuration
observability:
  phoenix_enabled: false  # Enable Arize Phoenix tracing
  phoenix_endpoint: "http://localhost:6006"  # Phoenix base endpoint (OTLP traces sent to /v1/traces)
  project_name: "coloraria-rag"  # Project name in Phoenix UI

# Workflow Configuration
workflow:
  use_langgraph: true  # Use LangGraph for chat workflow orchestration

# Agent Collector Configuration
agent:
  provider: "azure_openai"  # gemini | openai | azure_openai (can differ from main llm provider)
  max_iterations: 5     # Maximum agent loop iterations
  model: "gpt-5-mini"  # Model name for Gemini, or deployment name for Azure OpenAI
  temperature: 1      # Lower temperature for more focused tool use
  # azure_api_version: "2024-12-01-preview"  # For Azure OpenAI


# Benchmark Configuration
benchmark:
  model: "gemini-2.5-flash" # Independent model for benchmarks
  temperature: 1          # Benchmarks usually require lower temperature

# Beta Testing Configuration
beta_testing:
  enabled: true  # Global test mode flag - set to true for beta cohort
  
  # Token Economy
  initial_tokens: 15  # Tokens for new test users
  refill_tokens: 10   # Tokens granted per survey completion
  
  # Survey Configuration (4 rating questions 1-5, 1 open question)
  survey_questions:
    - "¿Qué tan útil fue la respuesta del asistente? (1-5)"
    - "¿Qué tan claras fueron las citas a fuentes legales? (1-5)"
    - "¿Qué tan fácil fue entender la respuesta? (1-5)"
    - "¿Recomendarías este asistente a un colega? (1-5)"
    - "¿Cómo podemos mejorar nuestro producto? (respuesta abierta)"
