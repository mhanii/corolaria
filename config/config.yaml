# Coloraria Configuration
# All parameters externalized for easy tuning

llm:
  provider: "gemini"  # gemini | openai | anthropic
  model: "gemini-2.5-flash"  # or gemini-2.5-pro for more capability
  temperature: 1
  max_tokens: 10240000

retrieval:
  top_k: 10
  strategy: "vector"
  index_name: "article_embeddings"
  min_score: 0.0  # No minimum score filter by default
  max_refs: 0     # Maximum REFERS_TO articles to include per chunk

citations:
  format: "[{index}]"
  include_in_response: true
  max_citations: 20

conversation:
  max_history_messages: 10  # Keep last N messages for context
  max_context_tokens: 40000  # Approximate token limit for context

version_context:
  # How many next versions to include recursively (0 = none, -1 = all)
  next_version_depth: 0  # Get all subsequent versions
  # How many previous versions to include (0 = none)
  previous_version_depth: 0  # Get only immediate previous version

# Observability Configuration
observability:
  phoenix_enabled: false  # Enable Arize Phoenix tracing
  phoenix_endpoint: "http://localhost:6006"  # Phoenix base endpoint (OTLP traces sent to /v1/traces)
  project_name: "coloraria-rag"  # Project name in Phoenix UI

# Workflow Configuration
workflow:
  use_langgraph: true  # Use LangGraph for chat workflow orchestration

# Beta Testing Configuration
beta_testing:
  enabled: true  # Global test mode flag - set to true for beta cohort
  
  # Token Economy
  initial_tokens: 15  # Tokens for new test users
  refill_tokens: 10   # Tokens granted per survey completion
  
  # Survey Configuration (4 rating questions 1-5, 1 open question)
  survey_questions:
    - "¿Qué tan útil fue la respuesta del asistente? (1-5)"
    - "¿Qué tan claras fueron las citas a fuentes legales? (1-5)"
    - "¿Qué tan fácil fue entender la respuesta? (1-5)"
    - "¿Recomendarías este asistente a un colega? (1-5)"
    - "¿Cómo podemos mejorar nuestro producto? (respuesta abierta)"
